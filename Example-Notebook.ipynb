{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import MLcps","metadata":{},"id":"8e5f363b"},{"cell_type":"code","source":"#import MLcps\nfrom MLcps import getCPS","metadata":{},"execution_count":null,"outputs":[],"id":"b7faafb8"},{"cell_type":"markdown","source":"#### Create Input dataframe for MLcps","metadata":{},"id":"a1ebe4cf"},{"cell_type":"code","source":"import pandas as pd\nmetrics_list=[]\n\n#Metrics from SVC model (kernel=rbf)\nacc = 0.88811 #accuracy\nbacc = 0.86136 #balanced_accuracy\nprec = 0.86 #precision\nrec = 0.97727 #recall\nf1 = 0.91489 #F1\nmcc = 0.76677 #Matthews_correlation_coefficient\nmetrics_list.append([acc,bacc,prec,rec,f1,mcc])\n\n#Metrics from SVC model (kernel=linear)\nacc = 0.88811\nbacc = 0.87841\nprec = 0.90\nrec = 0.92045\nf1 = 0.91011\nmcc = 0.76235\nmetrics_list.append([acc,bacc,prec,rec,f1,mcc])\n\n#Metrics from KNN\nacc = 0.78811\nbacc = 0.82841\nprec = 0.80\nrec = 0.82\nf1 = 0.8911\nmcc = 0.71565\nmetrics_list.append([acc,bacc,prec,rec,f1,mcc])\n\nmetrics=pd.DataFrame(metrics_list,index=[\"SVM rbf\",\"SVM linear\",\"KNN\"],columns=[\"accuracy\",\"balanced_accuracy\",\"precision\",\"recall\",\"f1\",\"Matthews_correlation_coefficient\"])\nmetrics\n","metadata":{},"execution_count":null,"outputs":[],"id":"2f2f3bc8"},{"cell_type":"markdown","source":"#### Alternatively, load an example dataframe from MLcps","metadata":{},"id":"5a8c6cb0"},{"cell_type":"code","source":"#read input data (a dataframe) or load an example data\nmetrics_example=getCPS.sample_metrics()\nmetrics_example","metadata":{"scrolled":true},"execution_count":null,"outputs":[],"id":"58a030c2"},{"cell_type":"markdown","source":"## Calculate MLcps (Machine Learning cumulative performance score)","metadata":{},"id":"d230af2b"},{"cell_type":"code","source":"#calculate Machine Learning cumulative performance score\ncpsScore=getCPS.calculate(metrics)\ncpsScore","metadata":{"scrolled":true},"execution_count":null,"outputs":[],"id":"a450734e"},{"cell_type":"code","source":"#calculate Machine Learning cumulative performance score\ncpsScore=getCPS.calculate(metrics_example)\ncpsScore","metadata":{},"execution_count":null,"outputs":[],"id":"1c20ffe9"},{"cell_type":"markdown","source":"## Plot MLcps","metadata":{},"id":"675fcead"},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import plot\nimport plotly.io as pio\npio.renderers.default = 'iframe' \n\nfig = px.bar(cpsScore, x='Score', y='Algorithms',color='Score',labels={'MLcps Score'},\n             width=700,height=1000,text_auto=True)\n\nfig.update_xaxes(title_text=\"MLcps\")\nfig","metadata":{},"execution_count":null,"outputs":[],"id":"58fdeb85"},{"cell_type":"markdown","source":"## MLcps for GridSearch object\n##### Calculate MLcps using the mean test score of all the metrics available in the given GridSearch object and return an updated GridSearch object. Returned GridSearch object contains mean_test_MLcps and rank_test_MLcps arrays, which can be used to rank the models similar to any other metric.","metadata":{},"id":"6048d1e3"},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") #to ignore different version warning\n\n#read GridSearch object or load it from package\ngsObj=getCPS.sample_GridSearch_Object()\n\n#calculate Machine Learning cumulative performance score\ngsObj_updated=getCPS.calculate(gsObj)\ngsObj_updated\n","metadata":{},"execution_count":null,"outputs":[],"id":"869a6d26"},{"cell_type":"code","source":"#access MLcps\ngsObj_updated.cv_results_[\"mean_test_MLcps\"]","metadata":{},"execution_count":null,"outputs":[],"id":"787e92cb"},{"cell_type":"code","source":"#access rank array based on MLcps\ngsObj_updated.cv_results_[\"rank_test_MLcps\"]","metadata":{},"execution_count":null,"outputs":[],"id":"87b8b8b1"},{"cell_type":"markdown","source":"## Weighted MLcps\n##### Certain metrics are more significant than others in some cases. As an example, if the dataset is imbalanced, a high F1 score might be preferred to higher accuracy. A user can provide weights for metrics of interest while calculating MLcps in such a scenario. Weights should be a dictionary object where keys are metric names and values are corresponding weights. It can be passed as a parameter in getCPS.calculate() function.","metadata":{},"id":"f4fc3e7b"},{"cell_type":"code","source":"#read input data (a dataframe) or load an example data\nmetrics=getCPS.sample_metrics()\n\n#define weights\nweights={\"Accuracy\":0.75,\"F1\": 1.25}\n\n#calculate Machine Learning cumulative performance score\ncpsScore=getCPS.calculate(metrics,weights)\ncpsScore","metadata":{},"execution_count":null,"outputs":[],"id":"098754db"},{"cell_type":"code","source":"gsObj=getCPS.sample_GridSearch_Object()\n\n#define weights\nweights={\"accuracy\":0.75,\"f1\": 1.25}\n\n#calculate Machine Learning cumulative performance score\ngsObj_updated=getCPS.calculate(gsObj,weights)\ngsObj_updated.cv_results_[\"mean_test_MLcps\"]","metadata":{},"execution_count":null,"outputs":[],"id":"5d071e12"}]}