{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e5f363b",
   "metadata": {},
   "source": [
    "## Import MLcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7faafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import MLcps\n",
    "from MLcps import getCPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ebe4cf",
   "metadata": {},
   "source": [
    "#### Create Input dataframe for MLcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "metrics_list=[]\n",
    "\n",
    "#Metrics from SVC model (kernel=rbf)\n",
    "acc = 0.88811 #accuracy\n",
    "bacc = 0.86136 #balanced_accuracy\n",
    "prec = 0.86 #precision\n",
    "rec = 0.97727 #recall\n",
    "f1 = 0.91489 #F1\n",
    "mcc = 0.76677 #Matthews_correlation_coefficient\n",
    "metrics_list.append([acc,bacc,prec,rec,f1,mcc])\n",
    "\n",
    "#Metrics from SVC model (kernel=linear)\n",
    "acc = 0.88811\n",
    "bacc = 0.87841\n",
    "prec = 0.90\n",
    "rec = 0.92045\n",
    "f1 = 0.91011\n",
    "mcc = 0.76235\n",
    "metrics_list.append([acc,bacc,prec,rec,f1,mcc])\n",
    "\n",
    "#Metrics from KNN\n",
    "acc = 0.78811\n",
    "bacc = 0.82841\n",
    "prec = 0.80\n",
    "rec = 0.82\n",
    "f1 = 0.8911\n",
    "mcc = 0.71565\n",
    "metrics_list.append([acc,bacc,prec,rec,f1,mcc])\n",
    "\n",
    "metrics=pd.DataFrame(metrics_list,index=[\"SVM rbf\",\"SVM linear\",\"KNN\"],columns=[\"accuracy\",\"balanced_accuracy\",\"precision\",\"recall\",\"f1\",\"Matthews_correlation_coefficient\"])\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c6cb0",
   "metadata": {},
   "source": [
    "#### Alternatively, load an example dataframe from MLcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a030c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read input data (a dataframe) or load an example data\n",
    "metrics_example=getCPS.sample_metrics()\n",
    "metrics_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230af2b",
   "metadata": {},
   "source": [
    "## Calculate MLcps (Machine Learning cumulative performance score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450734e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calculate Machine Learning cumulative performance score\n",
    "cpsScore=getCPS.calculate(metrics)\n",
    "cpsScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Machine Learning cumulative performance score\n",
    "cpsScore=getCPS.calculate(metrics_example)\n",
    "cpsScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fcead",
   "metadata": {},
   "source": [
    "## Plot MLcps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe' \n",
    "\n",
    "fig = px.bar(cpsScore, x='Score', y='Algorithms',color='Score',labels={'MLcps Score'},\n",
    "             width=700,height=1000,text_auto=True)\n",
    "\n",
    "fig.update_xaxes(title_text=\"MLcps\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048d1e3",
   "metadata": {},
   "source": [
    "## MLcps for GridSearch object\n",
    "##### Calculate MLcps using the mean test score of all the metrics available in the given GridSearch object and return an updated GridSearch object. Returned GridSearch object contains mean_test_MLcps and rank_test_MLcps arrays, which can be used to rank the models similar to any other metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #to ignore different version warning\n",
    "\n",
    "#read GridSearch object or load it from package\n",
    "gsObj=getCPS.sample_GridSearch_Object()\n",
    "\n",
    "#calculate Machine Learning cumulative performance score\n",
    "gsObj_updated=getCPS.calculate(gsObj)\n",
    "gsObj_updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access MLcps\n",
    "gsObj_updated.cv_results_[\"mean_test_MLcps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access rank array based on MLcps\n",
    "gsObj_updated.cv_results_[\"rank_test_MLcps\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc3e7b",
   "metadata": {},
   "source": [
    "## Weighted MLcps\n",
    "##### Certain metrics are more significant than others in some cases. As an example, if the dataset is imbalanced, a high F1 score might be preferred to higher accuracy. A user can provide weights for metrics of interest while calculating MLcps in such a scenario. Weights should be a dictionary object where keys are metric names and values are corresponding weights. It can be passed as a parameter in getCPS.calculate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098754db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input data (a dataframe) or load an example data\n",
    "metrics=getCPS.sample_metrics()\n",
    "\n",
    "#define weights\n",
    "weights={\"Accuracy\":0.75,\"F1\": 1.25}\n",
    "\n",
    "#calculate Machine Learning cumulative performance score\n",
    "cpsScore=getCPS.calculate(metrics,weights)\n",
    "print(cpsScore)\n",
    "\n",
    "#Plot MLcps\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe' \n",
    "\n",
    "fig = px.bar(cpsScore, x='Score', y='Algorithms',color='Score',labels={'MLcps Score'},\n",
    "             width=700,height=1000,text_auto=True)\n",
    "\n",
    "fig.update_xaxes(title_text=\"MLcps\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d071e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsObj=getCPS.sample_GridSearch_Object()\n",
    "\n",
    "#define weights\n",
    "weights={\"accuracy\":0.75,\"f1\": 1.25}\n",
    "\n",
    "#calculate Machine Learning cumulative performance score\n",
    "gsObj_updated=getCPS.calculate(gsObj,weights)\n",
    "gsObj_updated.cv_results_[\"mean_test_MLcps\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
